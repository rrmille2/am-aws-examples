{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Textract and Amazon Comprehend AI Services\n",
    "### Example on extracting insights from a PDF Document\n",
    "\n",
    "\n",
    "## Contents \n",
    "1. [Background](#Background)\n",
    "1. [Notes and Configuration](#Notes-and-Configuration)\n",
    "1. [Functions](#Functions)\n",
    "1. [Amazon Textract](#Amazon-Textract)\n",
    "1. [Amazon Comprehend](#Amazon-Comprehend)\n",
    "1. [Key Phrase Extraction](#Key-Phrase-Extraction)\n",
    "1. [Sentiment Analysis](#Sentiment-Analysis)\n",
    "1. [Entity Recognition](#Entity-Recognition)\n",
    "1. [PII Entity Recognition](#PII-Entity-Recognition)\n",
    "1. [Topic Modeling](#Topic-Modeling)\n",
    "\n",
    "\n",
    "  \n",
    "## Background\n",
    "The goal of this exercise is to learn some insights from an existing PDF document. This is done by using Amazon Textract to extract the text from the document. This text is then analyzed by several Amazon Comprehend services to produce some insights about the document.  \n",
    "\n",
    "The PDF document used in this example is a compiled list of tweets or other social media posts. Each post is separated by a URL that points to that posting. When the text is extracted from the PDF document, the text is re-assembled into a single line of text which is the full text of the tweet or post. The resulting text file contains one tweet/post per line.\n",
    "\n",
    "## Notes and Configuration\n",
    "* Kernel `Python 3 (Data Science)` works well with this notebook\n",
    "* The CSV results files produced by this script use the pipe '|' symbol as a delimiter. When viewing these files in SageMaker Studio, be sure and change the Delimiter to 'pipe'.\n",
    "\n",
    "\n",
    "#### Regarding IAM Roles and Permissions:\n",
    "\n",
    "Within SageMaker Studio, each SageMaker User has an IAM Role known as the `SageMaker Execution Role`. Each Notebook for this user will run with this Role and the Permissions specified by this Role. The name of this Role can be found in the Details section of each SageMaker User in the AWS Console.\n",
    "\n",
    "For the code which runs in this notebook, the `SageMaker Execution Role` needs additional permissions to allow it to use Amazon Textract and Amazon Comprehend. In the AWS Console, navigate to the IAM service and add these two services to your SageMaker Execution Role:\n",
    "- AmazonTextractFullAccess\n",
    "- AmazonComprehendFullAccess\n",
    "\n",
    "Also, an Amazon Comprehend service Role needs to be created to grant Amazon Comprehend read access to your input data.  \n",
    "When creating this new Role, the default Policies are sufficient (i.e., no other Policies need to be added/modified).\n",
    "\n",
    "Lastly, the `SageMaker Execution Role` must be allowed to Pass the Comprehend Service Role. To allow this, you must attach a Policy to the `SageMaker Execution Role`. Below, the Resource entry is the ARN of the Comprehend service Role which you created. You can either create this as a new Policy and attach it or add it as an in-line Policy.\n",
    "\n",
    "    {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"iam:GetRole\",\n",
    "                    \"iam:PassRole\"\n",
    "                ],\n",
    "                \"Resource\": \"arn:aws:iam::810190279255:role/amComprehendServiceRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "import boto3\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Set some variables that will be used throughout this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'us-east-1'\n",
    "\n",
    "# change this to an existing S3 bucket in your AWS account\n",
    "bucket = 'am-buck1'\n",
    "\n",
    "# this is where the various analysis results files will be stored on the local file system of this SageMaker instance\n",
    "results_dir = './results'\n",
    "!mkdir -p $results_dir\n",
    "\n",
    "# the pdf file to be analyzed by Textract\n",
    "textract_src_filename = 'Alabama2.pdf'\n",
    "\n",
    "# the name of the file where the JSON results from Textract are saved\n",
    "json_textract_results_filename = f'{results_dir}/textract-results.json'\n",
    "\n",
    "# the post-processed results of the JSON results\n",
    "textract_results_filename = f'{results_dir}/textract-results.txt'\n",
    "\n",
    "# the results of Amazon Comprehend - Key Phrases detection\n",
    "comprehend_keyphrases_results_filename = f'{results_dir}/comp-keyphrases.csv'\n",
    "\n",
    "# the results of Amazon Comprehend - Sentiment Analysis\n",
    "comprehend_sentiments_results_filename = f'{results_dir}/comp-sentiment.csv'\n",
    "\n",
    "# the results of Amazon Comprehend - Entities Detection\n",
    "comprehend_entities_results_filename = f'{results_dir}/comp-entities.csv'\n",
    "\n",
    "# the results of Amazon Comprehend - Entities Detection\n",
    "comprehend_pii_entities_results_filename = f'{results_dir}/comp-pii_entities.csv'\n",
    "\n",
    "# the results of Amazon Comprehend - Topics Detection\n",
    "comprehend_topics_results_filename = f'{results_dir}/comp-topics.csv'\n",
    "\n",
    "# this is the IAM Role that defines which permissions this SageMaker instance has\n",
    "sm_execution_role = get_execution_role()\n",
    "\n",
    "# why is this IAM Role needed? See the Notes at the beginning of this Notebook for an explanation\n",
    "# replace the string below with the ARN of the Amazon Comprehend Service Role that you create in your AWS account\n",
    "comprehend_role = 'arn:aws:iam::810190279255:role/amComprehendServiceRole'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Functions\n",
    "The following functions provide a wrapper around the actual API calls for Amazon Textract and Amazon Comprehend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "FixupString()\n",
    "Input: string\n",
    "Returns: lowercase string with punctuation and stop words removed\n",
    "'''\n",
    "def FixupString(s):\n",
    "    punc = '!()-[]{};:\\,<>./?@#$%^&*_~\"\\''\n",
    "    \n",
    "    # these are the English stop words from the NLTK package\n",
    "    stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', \n",
    "                 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \n",
    "                 \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', \n",
    "                 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', \n",
    "                 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', \n",
    "                 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', \n",
    "                 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', \n",
    "                 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', \n",
    "                 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', \n",
    "                 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \n",
    "                 \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", \n",
    "                 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \n",
    "                 \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"\n",
    "                ]\n",
    "    \n",
    "    # remove punctuation\n",
    "    for c in punc: \n",
    "        if c in s: \n",
    "            s = s.replace(c, '')\n",
    "        \n",
    "    # convert to lowercase\n",
    "    s = s.lower()\n",
    "    \n",
    "    # remove stop words\n",
    "    words = s.split()\n",
    "    for w in words:\n",
    "        if w in stopwords:\n",
    "            words.remove(w)\n",
    " \n",
    "    # put string back together\n",
    "    s = ' '.join(words)\n",
    "    \n",
    "    return(s)\n",
    "\n",
    "'''\n",
    "CalcFrequencies()\n",
    "Input: dict with keys and numeric values\n",
    "Returns: dict with the same keys and numeric frequency\n",
    "'''\n",
    "def CalcFrequencies(di):\n",
    "    \n",
    "    freq = {}\n",
    "    \n",
    "    sum = 0\n",
    "    for d in di:\n",
    "        sum += di[d]\n",
    "    \n",
    "    for d in di:\n",
    "        freq[d] = di[d]/sum\n",
    "\n",
    "    return freq\n",
    "\n",
    "'''\n",
    "StartTextractJob()\n",
    "Input: textract-boto3-client, dict with keys and numeric values\n",
    "Returns: dict with the same keys and numeric frequency\n",
    "'''\n",
    "def StartTextractJob(client, bucket, src_filename):\n",
    "    response = client.start_document_text_detection(\n",
    "    DocumentLocation={\n",
    "        'S3Object': {\n",
    "            'Bucket': bucket,\n",
    "            'Name': src_filename\n",
    "        }\n",
    "    })\n",
    "\n",
    "    return response[\"JobId\"]\n",
    "\n",
    "\n",
    "'''\n",
    "GetTextractJobResults()\n",
    "Input: textract-boto3-client, textract job id\n",
    "Returns: textract results\n",
    "'''\n",
    "def GetTextractJobResults(client, jobId):\n",
    "\n",
    "    pages = []\n",
    "    seconds_ctr = 0\n",
    "    response = client.get_document_text_detection(JobId=jobId)\n",
    "    print('working.', end='')\n",
    "    while(seconds_ctr < 600):\n",
    "        print('.', end='')\n",
    "        response = client.get_document_text_detection(JobId=jobId)\n",
    "        if response['JobStatus'] == 'IN_PROGRESS':\n",
    "            time.sleep(2)\n",
    "            seconds_ctr += 2\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    if response['JobStatus'] == 'SUCCEEDED':\n",
    "        while(True):\n",
    "            pages.append(response)\n",
    "            nextToken = None\n",
    "            if('NextToken' in response):\n",
    "                nextToken = response['NextToken']\n",
    "                response = client.get_document_text_detection(JobId=jobId, NextToken=nextToken)\n",
    "\n",
    "            if nextToken == None:\n",
    "                break\n",
    "                \n",
    "    print('\\n')\n",
    "    return pages\n",
    "\n",
    "'''\n",
    "SaveTextractResults()\n",
    "Input: Textract results (JSON), and output filename\n",
    "Output: a file with the postprocessed results\n",
    "Returns: nothing\n",
    "'''\n",
    "def SaveTextractResults(response, filename):\n",
    "    with open(filename, 'w') as fd:\n",
    "        # iterate through the Textract responses, looking for the LINE and WORD entries\n",
    "        for resp in response:\n",
    "            for blk in resp['Blocks']:\n",
    "                if blk['BlockType'] in ['LINE', 'WORD']:\n",
    "                    # if 'http' is found at the beginning of the line, we assume a new paragraph of text will be started\n",
    "                    loc = blk['Text'].find('https')\n",
    "                    if loc >= 0 and loc <= 2:\n",
    "                        fd.write('\\n')\n",
    "                    else:\n",
    "                        fd.write('%s ' % blk['Text'])\n",
    "\n",
    "\n",
    "'''\n",
    "GetKeyPhrases()\n",
    "Input: Boto3 client for Comprehend, text file (each line will be analyzed for key phrases)\n",
    "Output: a file with the postprocessed results\n",
    "Returns: dict with key phrases (as the key) and counts\n",
    "'''\n",
    "def GetKeyPhrases(client, infile):\n",
    "\n",
    "    line_ctr = 0\n",
    "    print('working.', end='')\n",
    "    \n",
    "    # keep a running total of the various sentiments\n",
    "    result_ctr = {}\n",
    "\n",
    "    with open(infile) as fd:\n",
    "        lines = fd.readlines()\n",
    "        for line in lines:\n",
    "            line_ctr += 1\n",
    "            if line_ctr % 5 == 0:\n",
    "                print('.', end='')\n",
    "            if len(line) > 1:\n",
    "                # maximum text length for Comprehend Key Phrases is 5,000 characters\n",
    "                line = line[:4998]\n",
    "                line = line.replace('|', ' ')           \n",
    "                response = client.detect_key_phrases(Text=line, LanguageCode='en')\n",
    "                for keyphrase in response['KeyPhrases']:\n",
    "                    kp = keyphrase['Text']\n",
    "                    if kp in result_ctr:\n",
    "                        result_ctr[kp] += 1\n",
    "                    else:\n",
    "                        result_ctr[kp] = 1\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    # sort the dictionary by values\n",
    "    sorted_ctr = dict(sorted(result_ctr.items(), key=lambda x: x[1], reverse=True))\n",
    "    return sorted_ctr\n",
    "\n",
    "\n",
    "'''\n",
    "GetSentiments()\n",
    "Input: Boto3 client for Comprehend, text file (each line will be analyzed for sentiment), output file name\n",
    "Output: file with each line and its sentiment\n",
    "Returns: dict with sentiment types and counts\n",
    "'''\n",
    "def GetSentiments(client, infile, outfile):\n",
    "\n",
    "    line_ctr = 0\n",
    "    print('working.', end='')\n",
    "    \n",
    "    # keep a running total of the various sentiments\n",
    "    result_ctr = {}\n",
    "\n",
    "    with open(infile) as fd:\n",
    "        lines = fd.readlines()\n",
    "    \n",
    "    with open(outfile, 'w') as fd:\n",
    "        for line in lines:\n",
    "            line_ctr += 1\n",
    "            if line_ctr % 5 == 0:\n",
    "                print('.', end='')\n",
    "            # maximum text length for Comprehend Sentiment is 5,000 characters\n",
    "            line = line[:4998]\n",
    "            line = line.replace('|', ' ')           \n",
    "            if len(line) > 1:\n",
    "                response = client.detect_sentiment(Text=line, LanguageCode='en')\n",
    "                sentiment = response['Sentiment']\n",
    "                if sentiment in result_ctr:\n",
    "                    result_ctr[sentiment] += 1\n",
    "                else:\n",
    "                    result_ctr[sentiment] = 1\n",
    "                fd.write('%s|%s' % (sentiment, line))\n",
    "    \n",
    "    print('\\n')\n",
    "\n",
    "    # sort the dictionary by values\n",
    "    sorted_ctr = dict(sorted(result_ctr.items(), key=lambda x: x[1], reverse=True))\n",
    "    return sorted_ctr\n",
    "\n",
    "\n",
    "'''\n",
    "GetEntities()\n",
    "Input: text file (each line will be analyzed for entities), output file name\n",
    "Output: file with each line and its entity\n",
    "Returns: dict with entity types and counts\n",
    "'''\n",
    "def GetEntities(client, infile, outfile):\n",
    "\n",
    "    line_ctr = 0\n",
    "    print('working.', end='')\n",
    "    \n",
    "    # keep a running total of the various sentiments\n",
    "    result_ctr = {}\n",
    "\n",
    "    with open(infile) as fd:\n",
    "        lines = fd.readlines()\n",
    "\n",
    "    with open(outfile, 'w') as fd:\n",
    "        for line in lines:\n",
    "            line_ctr += 1\n",
    "            if line_ctr % 5 == 0:\n",
    "                print('.', end='')\n",
    "            line = line[:4998]\n",
    "            line = line.replace('|', ' ')                       \n",
    "            if len(line) > 1:\n",
    "                # maximum text length for Comprehend Entities is 5,000 characters\n",
    "                response = client.detect_entities(Text=line, LanguageCode='en')\n",
    "                for entity in response['Entities']:\n",
    "                    etype = entity['Type']\n",
    "                    if etype in result_ctr:\n",
    "                        result_ctr[etype] += 1\n",
    "                    else:\n",
    "                        result_ctr[etype] = 1\n",
    "                    fd.write('%s|%s\\n' % (etype, entity['Text']))\n",
    "    print('\\n')\n",
    "    \n",
    "    # sort the dictionary by values\n",
    "    sorted_ctr = dict(sorted(result_ctr.items(), key=lambda x: x[1], reverse=True))\n",
    "    return sorted_ctr\n",
    "\n",
    "\n",
    "'''\n",
    "GetPIIEntities()\n",
    "Input: text file (each line will be analyzed for PII entities), output file name\n",
    "Output: file with each line and its entity type\n",
    "Returns: dict with PII entity types and counts\n",
    "'''\n",
    "def GetPIIEntities(client, infile, outfile):\n",
    "\n",
    "    line_ctr = 0\n",
    "    print('working.', end='')\n",
    "    \n",
    "    # keep a running total of the various sentiments\n",
    "    result_ctr = {}\n",
    "\n",
    "    with open(infile) as fd:\n",
    "        lines = fd.readlines()\n",
    "\n",
    "    with open(outfile, 'w') as fd:\n",
    "        for line in lines:\n",
    "            line_ctr += 1\n",
    "            if line_ctr % 5 == 0:\n",
    "                print('.', end='')\n",
    "            # maximum text length for Comprehend Entities is 5,000 characters\n",
    "            line = line[:4998]\n",
    "            line = line.replace('|', ' ')           \n",
    "            if len(line) > 1:\n",
    "                response = client.detect_pii_entities(Text=line, LanguageCode='en')\n",
    "                for entity in response['Entities']:\n",
    "                    etype = entity['Type']\n",
    "                    if etype in result_ctr:\n",
    "                        result_ctr[etype] += 1\n",
    "                    else:\n",
    "                        result_ctr[etype] = 1\n",
    "                    fd.write('%s|%s\\n' % (etype, line[entity['BeginOffset']:entity['EndOffset']]))\n",
    "                    \n",
    "    print('\\n')\n",
    "    \n",
    "    # sort the dictionary by values\n",
    "    sorted_ctr = dict(sorted(result_ctr.items(), key=lambda x: x[1], reverse=True))\n",
    "    return sorted_ctr\n",
    "\n",
    "\n",
    "'''\n",
    "StartTopicsAnalysisJob()\n",
    "Input: Boto3 Comprehend client, S3 bucket, Role, and the file containing the text to be analyzed\n",
    "Returns: Job ID\n",
    "'''\n",
    "def StartTopicsAnalysisJob(client, bucket, role, infile):\n",
    "    # create a unique Job Name\n",
    "    JobName = 'MyJobName-%d' % (time.time())\n",
    "\n",
    "    request = {\n",
    "       \"ClientRequestToken\": \"string\",\n",
    "       \"DataAccessRoleArn\": '%s' % (role),\n",
    "       \"InputDataConfig\": { \n",
    "          \"InputFormat\": \"ONE_DOC_PER_FILE\",\n",
    "          \"S3Uri\": 's3://%s/%s' % (bucket, infile)\n",
    "       },\n",
    "       \"JobName\": JobName,\n",
    "       \"OutputDataConfig\": { \n",
    "          \"S3Uri\": 's3://%s' % (bucket)\n",
    "       }\n",
    "    }\n",
    "\n",
    "    # create the comprehend analysis job\n",
    "    job = client.start_topics_detection_job(**request)\n",
    "    return job['JobId']\n",
    "\n",
    "'''\n",
    "GetTopicsAnalysisJob()\n",
    "Input: Boto3 Comprehend client, JobId\n",
    "Returns: \n",
    "'''\n",
    "def GetTopics(client, jobid):\n",
    "    seconds_ctr = 0\n",
    "    print('working.', end='')\n",
    "    \n",
    "    while(seconds_ctr < 3600):\n",
    "        response = client.describe_topics_detection_job(JobId=jobid)\n",
    "        status = response['TopicsDetectionJobProperties']['JobStatus']\n",
    "        if status == 'IN_PROGRESS' or status == 'SUBMITTED':\n",
    "            print('.', end='')\n",
    "            seconds_ctr += 3\n",
    "            time.sleep(3)\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    print('\\n')\n",
    "    return response['TopicsDetectionJobProperties']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Amazon Textract\n",
    "Amazon Textract is a machine learning service that automatically extracts text, handwriting and data from scanned documents that goes beyond simple optical character recognition (OCR) to identify, understand, and extract data from forms and tables.  \n",
    "  \n",
    "In the next few cells the following steps will be performed:\n",
    "1. A specified PDF document will be uploaded to Amazon S3 and analyzed by Amazon Textract.  \n",
    "1. The result of this analysis is a JSON file with each element containing details about a specific instance of text in the PDF.  \n",
    "1. This JSON file is copied from S3 to this local SageMaker instance.  \n",
    "1. The JSON file is then read and post-processed to produce a text file with one tweet (or other social media post) per line.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a boto3 session\n",
    "# this session will be used for the remainder of this notebook\n",
    "session = boto3.Session(region_name=region)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Textract job at Fri Apr 16 20:14:28 2021\n",
      "JobId: 3b805af6818cac67cb193e86a514d21268f82cba584f50b308658e0d633b9c15\n"
     ]
    }
   ],
   "source": [
    "# create the Textract Job\n",
    "textract_client = session.client('textract')\n",
    "jobId = StartTextractJob(textract_client, bucket, textract_src_filename)\n",
    "print('Started Textract job at %s' % (time.ctime()))\n",
    "print('JobId: %s' % (jobId))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working..................................................................................\n",
      "\n",
      "CPU times: user 4.69 s, sys: 119 ms, total: 4.81 s\n",
      "Wall time: 2min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# wait for job to complete and get the results\n",
    "results = GetTextractJobResults(textract_client, jobId)\n",
    "\n",
    "# save the entire results set to a local file\n",
    "# this file isn't used in the remaining example, but you can open this JSON file in your Jupyter Notebook and view the elements returned by Textract\n",
    "with open(json_textract_results_filename, 'w') as fd:\n",
    "    json.dump(results, fd)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See results file: ./results/textract-results.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SaveTextractResults(results, textract_results_filename)\n",
    "print('See results file: %s\\n' % textract_results_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Amazon Comprehend\n",
    "Amazon Comprehend is a natural language processing (NLP) service that uses machine learning to discover insights from text. The service provides APIs for Keyphrase Extraction, Sentiment Analysis, Entity Recognition, Topic Modeling, and Language Detection so you can easily integrate natural language processing into your applications. The following cells will walk through several examples of how to use the API.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Phrase Extraction\n",
    "Use Amazon Comprehend to extract Key Phrases in the text from the Textract analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the comprehend boto3 client (from the existing boto3 session)\n",
    "comp_client = session.client('comprehend')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Comprehend Key Phrases Analysis job\n",
      "working...................................................................................\n",
      "\n",
      "See results file: ./results/comp-keyphrases.csv\n"
     ]
    }
   ],
   "source": [
    "print('Started Comprehend Key Phrases Analysis job')\n",
    "keyphrase_counts = GetKeyPhrases(comp_client, textract_results_filename)\n",
    "\n",
    "# calculate the frequency of each key phrase\n",
    "freq = CalcFrequencies(keyphrase_counts)\n",
    "\n",
    "# the results file is in csv format and includes the raw counts and the frequency\n",
    "with open(comprehend_keyphrases_results_filename, 'w') as fd:\n",
    "    fd.write('key_phrase|count|frequency\\n')\n",
    "    for kp in keyphrase_counts:        \n",
    "        fd.write('%s|%d|%.4f\\n' % (kp, keyphrase_counts[kp], freq[kp]))\n",
    "\n",
    "print('See results file: %s' % (comprehend_keyphrases_results_filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sentiment Analysis\n",
    "Use Amazon Comprehend to determine the Sentiment of each line of text from the Textract analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Comprehend Sentiment Analysis job\n",
      "working...................................................................................\n",
      "\n",
      "See results file: ./results/comp-sentiment.csv\n",
      "\n",
      "Frequencies:\n",
      "NEUTRAL: 0.72\n",
      "NEGATIVE: 0.26\n",
      "POSITIVE: 0.02\n",
      "MIXED: 0.00\n"
     ]
    }
   ],
   "source": [
    "print('Started Comprehend Sentiment Analysis job')\n",
    "sentiments = GetSentiments(comp_client, textract_results_filename, comprehend_sentiments_results_filename)\n",
    "print('See results file: %s\\n' % (comprehend_sentiments_results_filename))\n",
    "\n",
    "freq = CalcFrequencies(sentiments)\n",
    "print('Frequencies:')\n",
    "for d in sentiments:\n",
    "    print('%s: %.2f' % (d, freq[d]))        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Entity Recognition\n",
    "Use Amazon Comprehend to detect Entities in the text from the Textract analysis.  \n",
    "What are the type of Entities?\n",
    "* PERSON, ORGANIZATION, DATE, QUANTITY, LOCATION, TITLE, COMMERCIAL_ITEM, EVENT, OTHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Comprehend Entities Analysis job\n",
      "working...................................................................................\n",
      "\n",
      "See results file: ./results/comp-entities.csv\n",
      "\n",
      "Frequencies:\n",
      "PERSON: 0.31\n",
      "ORGANIZATION: 0.18\n",
      "DATE: 0.17\n",
      "QUANTITY: 0.14\n",
      "LOCATION: 0.09\n",
      "OTHER: 0.05\n",
      "TITLE: 0.02\n",
      "COMMERCIAL_ITEM: 0.02\n",
      "EVENT: 0.01\n"
     ]
    }
   ],
   "source": [
    "print('Started Comprehend Entities Analysis job')\n",
    "entities = GetEntities(comp_client, textract_results_filename, comprehend_entities_results_filename)\n",
    "print('See results file: %s\\n' % (comprehend_entities_results_filename))\n",
    "\n",
    "freq = CalcFrequencies(entities)\n",
    "print('Frequencies:')\n",
    "for d in entities:\n",
    "    print('%s: %.2f' % (d, freq[d]))        \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PII Entity Recognition\n",
    "Use Amazon Comprehend to detect PII Entities in the text from the Textract analysis.  \n",
    "What are the types of PII Entities?  \n",
    "* NAME, DATE-TIME, ADDRESS, USERNAME, URL, EMAIL, PHONE, CREDIT-DEBIT-EXPIRY, PASSWORD, AGE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Comprehend PII Entities Analysis job\n",
      "working...................................................................................\n",
      "\n",
      "See results file: ./results/comp-pii_entities.csv\n",
      "\n",
      "Frequencies:\n",
      "NAME: 0.34\n",
      "DATE_TIME: 0.30\n",
      "ADDRESS: 0.16\n",
      "USERNAME: 0.10\n",
      "URL: 0.06\n",
      "EMAIL: 0.05\n",
      "PHONE: 0.00\n",
      "CREDIT_DEBIT_EXPIRY: 0.00\n",
      "PASSWORD: 0.00\n",
      "AGE: 0.00\n"
     ]
    }
   ],
   "source": [
    "print('Started Comprehend PII Entities Analysis job')\n",
    "pii_entities = GetPIIEntities(comp_client, textract_results_filename, comprehend_pii_entities_results_filename)\n",
    "print('See results file: %s\\n' % (comprehend_pii_entities_results_filename))\n",
    "\n",
    "freq = CalcFrequencies(pii_entities)\n",
    "print('Frequencies:')\n",
    "for d in pii_entities:\n",
    "    print('%s: %.2f' % (d, freq[d]))        \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Topic Modeling\n",
    "Use Amazon Comprehend to extract Topics in the text from the Textract analysis.  \n",
    "\n",
    "In this example, we are running the analysis as an asynchronous job, so the results are stored in a file in the S3 bucket we specify.  \n",
    "This analysis may take up to 10 minutes to run.  \n",
    "\n",
    "In this example, the input is a single file. Each line in the file is considered a document. This is best for short documents, such as social media postings.  \n",
    "Each line must end with a line feed (LF, \\n), a carriage return (CR, \\r), or both (CRLF, \\r\\n).   \n",
    "\n",
    "The output results are two files:  \n",
    "*topic_terms.csv:*  A list of topics in the collection. For each topic, the list includes the top terms by topic according to their weight.  \n",
    "*doc-topics.csv:*   Lists the documents associated with a topic and the proportion of the document that is concerned with the topic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: results/textract-results.txt to s3://am-buck1/textract-results.txt\n"
     ]
    }
   ],
   "source": [
    "# put the file to be analyzed into the s3 bucket\n",
    "# in this example, this file is the results from running textract on a pdf\n",
    "s3dest = 's3://%s/%s' % (bucket, os.path.basename(textract_results_filename))\n",
    "!aws s3 cp $textract_results_filename $s3dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Comprehend Topics Analysis job at Fri Apr 16 20:18:55 2021\n",
      "JobId: f573bc7bcb05193bffe517e60be95991\n"
     ]
    }
   ],
   "source": [
    "# start the Amazon Comprehend Topics Analysis job\n",
    "jobId = StartTopicsAnalysisJob(comp_client, bucket, comprehend_role, textract_results_filename)\n",
    "print('Started Comprehend Topics Analysis job at %s' % (time.ctime()))\n",
    "print('JobId: %s' % (jobId))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working...................................................................................................................................................................\n",
      "\n",
      "COMPLETED\n",
      "download: s3://am-buck1/810190279255-TOPICS-f573bc7bcb05193bffe517e60be95991/output/output.tar.gz to results/output.tar.gz\n",
      "delete: s3://am-buck1/810190279255-TOPICS-f573bc7bcb05193bffe517e60be95991/output/output.tar.gz\n",
      "See the following files:\n",
      "-rw-r--r-- 1 root root 2568 Apr 16 20:25 ./results/topic-terms.csv\n",
      "-rw-r--r-- 1 root root 195 Apr 16 20:25 ./results/doc-topics.csv\n",
      "CPU times: user 738 ms, sys: 109 ms, total: 847 ms\n",
      "Wall time: 8min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = GetTopics(comp_client, jobId)\n",
    "print(results['JobStatus'])\n",
    "\n",
    "# the comprehend analysis results are in the s3 bucket, full path is S3Uri\n",
    "s3uri = results['OutputDataConfig']['S3Uri']\n",
    "basename = os.path.basename(s3uri)\n",
    "\n",
    "# copy the 'output.tar.gz' file from the s3 bucket to the results folder\n",
    "!aws s3 cp $s3uri $results_dir\n",
    "\n",
    "# remove the folder/file from s3\n",
    "!aws s3 rm $s3uri\n",
    "\n",
    "# extract the contents of this tarball, which are two files: topic-terms.csv, doc-topics.csv\n",
    "!(cd $results_dir; tar xzf $basename)\n",
    "!(cd $results_dir; rm -f $basename)\n",
    "\n",
    "print('See the following files:')\n",
    "!ls -l $results_dir/topic-terms.csv\n",
    "!ls -l $results_dir/doc-topics.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
